<!DOCTYPE html>



<html lang="en" 
  
    mode="light"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  
    <meta name="pv-cache-enabled" content="false">

    
  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="🍗 양념치킨 🍗" />
<meta name="author" content="KEJdev" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/page7/" />
<meta property="og:url" content="http://localhost:4000/page7/" />
<meta property="og:site_name" content="🍗 양념치킨 🍗" />
<meta property="og:type" content="website" />
<link rel="prev" href="http://localhost:4000/page6" />
<link rel="next" href="http://localhost:4000/page8" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="🍗 양념치킨 🍗" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"KEJdev"},"headline":"🍗 양념치킨 🍗","url":"http://localhost:4000/page7/"}</script>
<!-- End Jekyll SEO tag -->


  <title>🍗 양념치킨 🍗
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://www.favicon-generator.org/
-->



<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">
<link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">

<!-- <link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png">
<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"> -->
<!-- <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png">

<link rel="icon" type="image/png" sizes="192x192"  href="/assets/img/favicons/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"> -->

<link rel="manifest" href="/assets/img/favicons/manifest.json">
<meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'> 
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/assets/img/favicons/favicon.png">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
  

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"
    integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous">

  <!-- Font Awesome -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"
    integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ="
    crossorigin="anonymous">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">




  <!-- JavaScripts -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

  <script defer
    src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>

  <!--
  JS selector for site.
-->


  





<script defer src="/assets/js/dist/home.min.js"></script>






</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" style="width:190px" class="d-flex flex-column align-items-end">

  <div class="profile-wrapper text-center">
    <div style="display:flex">
      <div id="avatar" style="margin:0 auto 0 auto;">
        <a href="/" alt="avatar" class="mx-auto">
          
          <img style="max-width:100%;" src="/assets/img/favicons/mycat.jpg" alt="avatar" onerror="this.style.display='none'">
        </a>
      </div>
    </div>

    <div style="display:flex">
      <div style="margin:0 auto 0 auto; font-weight:550px;" >
        <a href="/">🍗 양념치킨 🍗</a>
      </div>
    </div>

    <div style="display:flex">
      <div class="site-subtitle font-italic"  style="margin:0 auto 0 auto;">고양이 좋아하는 개발자</div>
    </div>
  </div><!-- .profile-wrapper -->

  <ul class="w-100">
    <!-- home -->
    <li class="nav-item active">
      <a href="/" class="nav-link">
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/tabs/categories/" class="nav-link">
        <span>CATEGORIES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/tabs/archives/" class="nav-link">
        <span>ARCHIVES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/tabs/about/" class="nav-link">
        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    


  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->


<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->


<!-- Add attribute 'hide-bullet' to the checkbox list -->




<!-- return -->
<div class="row">
  <div class="col-12 col-lg-11 col-xl-8" style="margin:0 auto; max-width: 750px">
    <div id="page" class="post pb-5 pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4 mb-md-4">
    
      



<!-- Get pinned posts -->







<!-- Get default posts -->











<div id="post-list">



  <div class="post-preview">
    <h1>
      <a href="/posts/python-web5/">파이썬으로 모든 신문사 웹스크롤링 스크립트 만들기</a>
    </h1>

    <div class="post-content">
      <p>
        





        저번에 중앙일보와 한계례 신문사를 저번에 웹 스크롤링하는 것을 했습니다. 두번정도 해보니까 이제 슬슬 감이 잡히지 않던가요 ? 링크랑 기사내용 태그만 확인하면 스크롤링 되는 것을 확인 했으니, 이번엔 그냥 전체 신문사에서 선택해서 스크롤링하는 스크립트를 짜볼까합니다.

웹 스크롤링 함수 구현

우선 자신의 컴퓨터 user-agent를 확인해야합니다. ...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Mar 15, 2020,  8:00 PM +0800"
  >

  
  

  
    Mar 15, 2020
  

  <i class="unloaded">2020-03-15T20:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="저번에 중앙일보와 한계례 신문사를 저번에 웹 스크롤링하는 것을 했습니다. 두번정도 해보니까 이제 슬슬 감이 잡히지 않던가요 ? 링크랑 기사내용 태그만 확인하면 스크롤링 되는 것을 확인 했으니, 이번엔 그냥 전체 신문사에서 선택해서 스크롤링하는 스크립트를 짜볼까합니다.

웹 스크롤링 함수 구현

우선 자신의 컴퓨터 user-agent를 확인해야합니다. 여기를 눌러 자신의 agent를 꼭 확인합니다.

우리는 메인 함수와 서브 함수 두가지를 우선 만들어야 합니다. 메인 함수는 스크롤링한 text를 리턴하는 함수를 만들고, 서브 함수는 두 가지 정도를 만들려고 합니다. 서브 함수는 기사 상세 url과 기사 text를 리스트를 append시키는 함수, 그리고 url를 입력받아 html로 변환하고 beautiful soup에서 사용할수 있도록 설정하는 함수를 만들겠습니다.

우선 서브 함수로 url를 받아서 html로 변화하는 함수를 만들겠습니다.

1
2
3
4
5
6
7
8
import urllib.request as rq
from bs4 import BeautifulSoup

def start(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',}
    url = rq.Request(url, headers = headers)
    res = rq.urlopen(url).read()
return BeautifulSoup(res, "html.parser")


그리고 기사 url과 기사 text를 리스트에 담는 함수를 작성합니다.

1
2
3
4
5
def news_fetch(url, tag):
    soup = start(url)

    for link in soup.select(tag[1]):
    result.append(link.get_text())


그리고 메인 함수인 스크롤링한 text를 리턴하는 함수를 작성합니다.

1
2
3
4
5
6
7
8
def fetch_list_url(url, tag):
    global result
    soup = start(url)

    for link in soup.select(tag[0]):
        result.append(link.get_text())
        print('link =' ,link['href'], link.get_text())
        news_fetch(link['href'], tag)


이제 가장 중요한 url링크와 기사 내용을 스크롤링해야 할 태그 부분을 가져옵니다.



1
2
result = []
url_list = ['http://search.daum.net/search?w=news&amp;q={search}&amp;spacing=0&amp;p={page}&amp;cp=16ZHEMAarrmZlVrZG3&amp;cpname=%EB%94%94%EC%A7%80%ED%84%B8%ED%83%80%EC%9E%84%EC%8A%A4', ["#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div &gt; div &gt; a", "#resizeContents &gt; div"]]




여기서 이제 주소 끝 부분에 ‘디지털 타임즈’가 아니라 각종 신문사를 넣어 링크를 여러개 만들어 전체 신문사를 스크롤링 스크립트를 작성하려고 합니다.

전체 신문사 기사 스크롤링

신문사를 선택해서 스크롤링 하는 최종 코드는 아래와 같습니다.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
import urllib.request as rq
from bs4 import BeautifulSoup
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from os import path
import re

def start(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',}
    url = rq.Request(url, headers = headers)
    res = rq.urlopen(url).read()
    return BeautifulSoup(res, "html.parser")

def fetch_list_url(url, tag):
    global result
    soup = start(url)
    for link in soup.select(tag[0]):
        result.append(link.get_text())
        print('link =', link['href'], link.get_text())
        news_fetch(link['href'], tag)

def news_fetch(url, tag):
    soup = start(url)
    for link in soup.select(tag[1]):
        result.append(link.get_text())

result = []
search_text = str(input("검색어를 입력하세요 : ").encode("utf-8"))[2:-1].replace('\\x', '%')

def numbers_to_strings():
    num = input('1 : 전자신문 \n2 : 디지털 타임즈 \n3 : 경향신문 \n4 : 중앙일보 \n5 : 동아일보 \n6 : 조선일보\n')

    switcher = {
        1: ['http://search.daum.net/search?w=news&amp;q={search}&amp;spacing=0&amp;p={page}&amp;cp=16yGc-mR1Rz5JT4-UZ&amp;cpname=%EC%A0%84%EC%9E%90%EC%8B%A0%EB%AC%B8&amp;DA=PGD', ["#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div &gt; div &gt; a", "#articleBody &gt; p"]],
        2: ['http://search.daum.net/search?w=news&amp;q={search}&amp;spacing=0&amp;p={page}&amp;cp=16ZHEMAarrmZlVrZG3&amp;cpname=%EB%94%94%EC%A7%80%ED%84%B8%ED%83%80%EC%9E%84%EC%8A%A4', ["#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div &gt; div &gt; a", "#resizeContents &gt; div"]],
        3: ['http://search.daum.net/search?w=news&amp;q={search}&amp;spacing=0&amp;p={page}&amp;cp=16bfGN9mQcFhOx4F5l&amp;cpname=%EA%B2%BD%ED%96%A5%EC%8B%A0%EB%AC%B8', ["#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div &gt; div &gt; a", "#container &gt; div.main_container &gt; div.art_cont &gt; div.art_body &gt; p"]],
        4: ['http://search.daum.net/search?nil_suggest=btn&amp;w=news&amp;cluster=y&amp;q={search}&amp;cp=16nfco03BTHhdjCcTS&amp;cpname=%EC%A4%91%EC%95%99%EC%9D%BC%EB%B3%B4&amp;p={page}',["#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div.cont_inner &gt; div &gt; a " ,"#article_body"]],
        5: ['http://search.daum.net/search?w=news&amp;nil_search=btn&amp;enc=utf8&amp;cluster=y&amp;cluster_page=1&amp;q=AI&amp;cp=16Et2OLVVtHab8gcjE&amp;cpname={search}&amp;DA=PGD&amp;p={page}',["#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div.cont_inner &gt; div &gt; a " , "div.article_txt "]],
        6: ['http://search.daum.net/search?w=news&amp;nil_search=btn&amp;enc=utf8&amp;cluster=y&amp;cluster_page=1&amp;q=AI&amp;cp=16EeZKAuilXKH5dzIt&amp;cpname={search}&amp;p={page}',["#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div.cont_inner &gt; div &gt; a ","div.par"]]
    }
    return switcher.get(int(num), "nothing")

url_list = numbers_to_strings()

for i in range(1, 2):
    url, tag = url_list[0].format(search=search_text, page=i), url_list[1]
    fetch_list_url(url, tag)
    
print(result)

f = open('data3.txt', 'w', encoding='UTF-8')
f.writelines(result)
f.close()




여기서 알아둬야 할 점은, 신문사 같은 경우는 웹 스크롤링하는 사람이 꽤 있다고 들어서 신문사들도 태그나 URL를 바꾸는 경우도 있다고 합니다. 그래서 현재 작성된 코드가 지금은 돌아가더라도 시간이 지난 뒤에는 안돌아갈 수 있으니 이 부분을 꼭 참고하셔서 작성하셔야합니다.
 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/python-web4/">J사 신문사 웹스크롤링 하기</a>
    </h1>

    <div class="post-content">
      <p>
        





        저번에 H사 신문사를 웹 스크롤링해봤습니다. 신문사마다 주소가 다르기 때문에 이번에는 J사 주소를 가지고 와서 저번이랑 똑같이 웹 스크롤링을 해보도록 하겠습니다.

J사 웹스크롤링

우선 중앙일보 신문사 링크를 가져와 보겠습니다.



저번과 마찬가지로 메모장에 복붙하면 암호화 된 주소 를 확인할 수있습니다.

import urllib.request
...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Mar 15, 2020,  7:00 PM +0800"
  >

  
  

  
    Mar 15, 2020
  

  <i class="unloaded">2020-03-15T19:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="저번에 H사 신문사를 웹 스크롤링해봤습니다. 신문사마다 주소가 다르기 때문에 이번에는 J사 주소를 가지고 와서 저번이랑 똑같이 웹 스크롤링을 해보도록 하겠습니다.

J사 웹스크롤링

우선 중앙일보 신문사 링크를 가져와 보겠습니다.



저번과 마찬가지로 메모장에 복붙하면 암호화 된 주소 를 확인할 수있습니다.

1
2
3
4
5
6
7
import urllib.request
from bs4 import BeautifulSoup
    
search_text = input("검색어를 입력하세요 : ").encode("utf-8")
search_text = str(search_text)[2:-1].replace('\\x', '%')

list_url = "hhttp://search.joins.com/JoongangNews?page=2&amp;Keyword=" + search_text + "&amp;SortType=New&amp;SearchCategoryType=JoongangNews"


여기서 저번과 마찬가지로 페이지 번호를 확인하여 for을 통해 스크롤링하겠습니다.



1
2
3
4
5
6
7
8
9
10
11
12
13
def fetch_list_url():
    params = []
    for i in range(2):
        list_url = "https://news.joins.com/Search/JoongangNews?page=" + str(i+1) + "&amp;Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;SortType=New&amp;SearchCategoryType=JoongangNews" 
        url = urllib.request.Request(list_url)
        res = urllib.request.urlopen(url).read().decode("utf-8")

        soup = BeautifulSoup(res, "html.parser")

        for link in soup.find_all(class_="headline mg"):
            for i in link: 
                params.append(i.get('href'))  # 신문자 URL
    return params


최종적으로 작성하면 아래와 같이 작성 할 수 있겠습니다.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
import urllib.request
from bs4 import BeautifulSoup

search_text = input("검색어를 입력하세요 : ").encode("utf-8")
search_text = str(search_text)[2:-1].replace('\\x', '%')

list_url = "hhttp://search.joins.com/JoongangNews?page=2&amp;Keyword=" + search_text + "&amp;SortType=New&amp;SearchCategoryType=JoongangNews"

def fetch_list_url():
    params = []
    for i in range(2):
        list_url = "https://news.joins.com/Search/JoongangNews?page=" + str(i+1) + "&amp;Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;SortType=New&amp;SearchCategoryType=JoongangNews"
        url = urllib.request.Request(list_url)
        res = urllib.request.urlopen(url).read().decode("utf-8")

        soup = BeautifulSoup(res, "html.parser")

        for link in soup.find_all(class_="headline mg"):
            for i in link:
                params.append(i.get('href'))  # 신문자 URL
    return params

def fetch_list_url2():
    params3 = []
    list_url = fetch_list_url()

    for i in range(len(list_url)):
        url = urllib.request.Request(list_url[i])
        res = urllib.request.urlopen(url).read().decode("utf-8")
        soup = BeautifulSoup(res, "html.parser")

        params1 = []
        params2 = []

        for link1, link2 in zip(soup.find_all('div', class_="byline"), soup.find_all('div', id="article_body")):
            params1.append(link1.get_text())
            params2.append(link2.get_text())

        for i1, i2 in zip(params1, params2):
            params3.append(i1.strip())
            params3.append(i2.strip())

    return params3

f = open('ydata3.txt', 'w', encoding='UTF-8')
f.writelines(fetch_list_url2())
f.close()

 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/python-web3/">H사 신문사 웹스크롤링 하기-3(검색어 입력받기)</a>
    </h1>

    <div class="post-content">
      <p>
        





        저번 포스팅에서는 기사 제목과 내용을 스크롤링하는 것을 했는데, 이번에는 스크롤링 하려고 검색어을 입력받아 자동으로 스크롤링하는 것을 짜보겠습니다.

H사 웹 스크롤링

저번에 가져온 암호화 된 주소를 살펴보면 아래와 같은데, 여기서 검색어를 삽입해야합니다.



def han_article():
    # 모듈 임포트
    import urllib...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Mar 15, 2020,  6:00 PM +0800"
  >

  
  

  
    Mar 15, 2020
  

  <i class="unloaded">2020-03-15T18:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="저번 포스팅에서는 기사 제목과 내용을 스크롤링하는 것을 했는데, 이번에는 스크롤링 하려고 검색어을 입력받아 자동으로 스크롤링하는 것을 짜보겠습니다.

H사 웹 스크롤링

저번에 가져온 암호화 된 주소를 살펴보면 아래와 같은데, 여기서 검색어를 삽입해야합니다.



1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
def han_article():
    # 모듈 임포트
    import urllib.request
    from bs4 import BeautifulSoup
    import re
    import os

    # 검색어 입력
    search_text = input("검색어를 입력하세요 : ").encode("utf-8")
    search_text = str(search_text)[2:-1].replace('\\x', '%')

    ##상세 기사 url   
    url_list = []
    for i in range(30):
        list_url = "http://search.hani.co.kr/Search?command=query&amp;keyword="+search_text+"&amp;media=news&amp;submedia=&amp;sort=d&amp;period=all&amp;datefrom=2020.01.01&amp;dateto=2020.03.22&amp;pageseq="+str(i)

        url = urllib.request.Request(list_url)  # url 요청
        res = urllib.request.urlopen(url).read().decode("utf-8")  # utf 파일로 decoding

        soup = BeautifulSoup(res, "html.parser")  # 전체 html

        ##url_list에 url 전체 담기
        for link in soup.find_all('dt'):  # dt 태그에 해당하는 모든 부분
            for i in link:  # resultset를 태그로 만드는 작업 
                            #dt 태그 밑에 있는 a 태그를 가져오기 위해 for loop
                url_list.append(i.get('href'))  # 태그가 되어야 get, get_text()를 쓸 수 있다.

    ##상세 기사    
    full_article = []

    # url 하나씩 불러오기
    for i in range(len(url_list)):
        url = urllib.request.Request(url_list[i])
        res = urllib.request.urlopen(url).read().decode("utf-8")

        # print(res)  # 위의 두가지 작업을 거치면 
        # 위의 url 의 html 문서를 res 변수에 담을수 있게 된다.
        
        soup = BeautifulSoup(res, "html.parser")

        day = []
        article = []

        for link1, link2 in zip(soup.find_all('p', class_="date-time"), soup.find_all('div', class_="text")):
            day.append(link1.get_text())
            article.append(link2.get_text())

        for i, j in zip(day, article):
            full_article.append(i.strip())
            full_article.append(j.strip())

    f = open('han_article.txt', 'w', encoding='UTF-8')
    f.writelines(full_article)
    f.close()

han_article()


스크롤링이 잘되는 것을 확인 할 수 있는데, 여기서 특정 기간만 선택해서 스크롤링하고 싶다면 위 그림을 참고해서 수정하면 될 것 같네요.
 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/python-web2/">H사 신문사 웹스크롤링 하기-2(기사 내용)</a>
    </h1>

    <div class="post-content">
      <p>
        





        지난 포스팅에서 기사 제목만을 스크롤링하는 것을 했는데, 이번에는 조금 더 나아가서 기사 내용까지 스크롤링해보도록 하겠습니다.

H사 웹 스크롤링 (기사 내용 스크롤링)

기사 내용을 스크롤링하기 위해 기사를 클릭 후 저번과 마찬가지로 F12를 눌러 기사 내용을 클릭합니다.



그럼 기사 내용이 div 태그에 text 클래스에 기사 내용이 있음을 확...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Mar 15, 2020,  5:00 PM +0800"
  >

  
  

  
    Mar 15, 2020
  

  <i class="unloaded">2020-03-15T17:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="지난 포스팅에서 기사 제목만을 스크롤링하는 것을 했는데, 이번에는 조금 더 나아가서 기사 내용까지 스크롤링해보도록 하겠습니다.

H사 웹 스크롤링 (기사 내용 스크롤링)

기사 내용을 스크롤링하기 위해 기사를 클릭 후 저번과 마찬가지로 F12를 눌러 기사 내용을 클릭합니다.



그럼 기사 내용이 div 태그에 text 클래스에 기사 내용이 있음을 확인할 수 있습니다. 여기서 기사 내용만을 클릭하면 언제 올라온 기사인지 모르니 기사 날짜도 함께 스크롤링합니다. 기사 날짜는 p 태그에 data-time 클래스가 있습니다.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
import urllib.request
from bs4 import BeautifulSoup

def fetch_list_url():

    # 현재 기사 URL
    list_url = "http://www.hani.co.kr/arti/economy/marketing/933198.html

    url = urllib.request.Request(list_url)
    res = urllib.request.urlopen(url).read().decode("utf-8")
    #print(res)  # 위의 두가지 작업을 거치면 위의 url 의 html 문서를 res 변수에 담을수 있게 된다.

    soup = BeautifulSoup(res, "html.parser")  
    params1 =[]
    params2 =[]

    for link1,link2 in zip(soup.find_all('p', class_="date-time"),soup.find_all('div', class_="text")):
        params1.append( link1.get_text() )
        params2.append( link2.get_text() )
    
    for i1,i2 in zip(params1,params2):
        print(i1.strip(),end=' ')
        print(i2.strip())

fetch_list_url()


그럼 여기서 현재 기사 URL만 스크롤링하지 않고 아래의 두가지 조건을 만족하는 웹 스크롤링을 작성해보겠습니다.


  1. 인공지능으로 검색한 페이지의 기사 URL을 가져오는 코드
  2. 그 상세 URL로 기사 날짜와 기사 내용을 검색하는 코드


저번 포스팅에서 사용한 코드와 오늘 작성한 코드를 합치면 간단하게 만들 수 있습니다.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
import urllib.request
from bs4 import BeautifulSoup

def fetch_list_url():
    params=[]
    
    for i in range(50):
        list_url = "http://search.hani.co.kr/Search?command=query&amp;keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;media=news&amp;submedia=&amp;sort=d&amp;period=all&amp;datefrom=2000.01.01&amp;dateto=2020.03.22&amp;pageseq="+str(i)
        url = urllib.request.Request(list_url)
        res = urllib.request.urlopen(url).read().decode("utf-8")

        soup = BeautifulSoup(res,"html.parser")  
        
        for link in soup.find_all('dt'):
            for i in link:
                params.append(i.get('href'))
    return params

def fetch_list_url2():

    list_url = fetch_list_url()  # 리스트를 한번에 받아오기
    
    for i in range(len(list_url)):
        url = urllib.request.Request(list_url[i])
        res = urllib.request.urlopen(url).read().decode("utf-8")
        
        soup = BeautifulSoup(res, "html.parser")  
        
        params1 =[]
        params2 =[]
    
        for link1,link2 in zip(soup.find_all('p', class_="date-time"),soup.find_all('div', class_="text")):
            params1.append( link1.get_text() )
            params2.append( link2.get_text() )
        
        for i1,i2 in zip(params1,params2):
            print(i1.strip(),end=' ')
            print(i2.strip())
	  
print(fetch_list_url2())


추가로 여기서 스크롤링한 기사를 메모장에 저장하고 난 최종 코드는 아래와 같이 작성할 수 있습니다.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
import urllib.request
from bs4 import BeautifulSoup

def fetch_list_url():
    txt_list = []
    for i in range(20):
        list_url = "http://search.hani.co.kr/Search?command=query&amp;keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;media=news&amp;submedia=&amp;sort=d&amp;period=all&amp;datefrom=2000.01.01&amp;dateto=2020.03.22&amp;pageseq="+str(i)
        url = urllib.request.Request(list_url)
        res = urllib.request.urlopen(url).read().decode("utf-8")
    
        soup = BeautifulSoup(res, "html.parser")  
        article_title_url = soup.find_all('dt')  

        for link in article_title_url:
            for i in link: 
                article_list_url = i.get('href')
                article_url = urllib.request.Request(article_list_url)
                article_res = urllib.request.urlopen(article_url).read().decode("utf-8")             

                soup = BeautifulSoup(article_res, "html.parser")
                article_content = soup.find_all('div', class_="text")
                article_date = soup.find_all('p', class_="date-time")              

                for i, j in zip(article_date,article_content):
                    txt_list.append(i.get_text(' ', strip = True)+' ')
                    txt_list.append(j.get_text(' ', strip = True).replace('\n',' ')+'\n')
        
    return txt_list

f = open('data.txt','w',encoding="UTF-8")
f.writelines(fetch_list_url())
f.close()


다음 포스팅에서는 검색어를 입력하면 자동으로 스크롤링하게 해보겠습니다.
 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/python-web1/">H사 신문사 웹스크롤링 하기-1(기사 제목)</a>
    </h1>

    <div class="post-content">
      <p>
        





        워드 클라우드나 특정 데이터를 수집하기 위해 자주 사용되는 웹 스크롤링에 대해 한번 알아보겠습니다. 보통 웹 스크롤링 할때는 a태그에 있는 것을 긁어서 사용합니다. 저는 오늘 특정 신문사에서 기사를 긁어오는 것을 한번 해보겠습니다.

H사 웹 스크롤링 (기사 제목만 스크롤링)

신문사 홈페이지로 들어가서 찾고자 하는 것을 검색한 후 url을 가져오겠습...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Mar 15, 2020,  4:00 PM +0800"
  >

  
  

  
    Mar 15, 2020
  

  <i class="unloaded">2020-03-15T16:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="워드 클라우드나 특정 데이터를 수집하기 위해 자주 사용되는 웹 스크롤링에 대해 한번 알아보겠습니다. 보통 웹 스크롤링 할때는 a태그에 있는 것을 긁어서 사용합니다. 저는 오늘 특정 신문사에서 기사를 긁어오는 것을 한번 해보겠습니다.

H사 웹 스크롤링 (기사 제목만 스크롤링)

신문사 홈페이지로 들어가서 찾고자 하는 것을 검색한 후 url을 가져오겠습니다.



여기서 알아야 할 점은 주소가 위 그림처럼 뜨지만, 주소를 메모장에 복붙하면 아래처럼 암호화 된 주소가 보여집니다. 그렇기 때문에 직접 URL를 건들어서 웹 스크롤링을 하기는 힘들다는 점을 알아두셔야합니다. 
이제 여기서 우리가 해야 댈것은 페이지 번호를 확인하고 나서 for문을 돌려서 웹 스크롤링 할 예정입니다. 우선 F12를 눌러 기사 제목을 클릭해서 기사 제목을 확인해보겠습니다.



F12를 누른 후 ctrl+ shift+c를 누른 후 기사 제목을 클릭하고 dt 태그 밑에 있는 a태그에서 링크와 기사 제목이 있는지를 확인합니다. 우리는 기사 제목을 스크롤링 해야하므로 a태그를 스크롤링해야합니다.

확인했으면 이제 페이지 번호를 확인하기 위해서 다음 페이지로 넘어갑니다.



주소를 보면 pageseq 라는 것이 생기면서 URL주소가 바뀌는 것을 확인할 수 있습니다. 그럼 이제 준비는 끝났습니다. 페이지 번호가 들어간 URL과 기사 제목이 어디에 위치 했는지를 확인했으니 웹 스크롤링을 할수 있겠네요.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
import urllib.request
from bs4 import BeautifulSoup  # 웹 스크롤링을 위한 모듈

def fetch_list_url():
    for i in range(100):
        list_url = "http://search.hani.co.kr/Search?command=query&amp;keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;media=news&amp;submedia=&amp;sort=d&amp;period=all&amp;datefrom=2000.01.01&amp;dateto=2020.03.22&amp;pageseq="+str(i)
        url = urllib.request.Request(list_url)
        res = urllib.request.urlopen(url).read().decode("utf-8")

        soup = BeautifulSoup(res, "html.parser")  
    
        for link in soup.find_all('dt'): # dt 태그 밑에 있는
            for i in link: # a 태그를 가져오기
                print(i.get('href'))  # 기사 URL
                print(i.get_text('href')) # 기사 제목

fetch_list_url()


웹 스크롤링이 잘되는 것을 확인할 수있습니다.



다음 포스팅에서는 신문사 기사 내용도 스크롤링하고 저장하는 것을 해보겠습니다.
 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/algorithm-kakao2/">2017 카카오 신입 공채 블라인드 시험문제(LRU알고리즘)</a>
    </h1>

    <div class="post-content">
      <p>
        





        두번째 카카오 알고리즘을 풀어보겠습니다.

LRU알고리즘(난이도 하)

지도개발팀에서 근무하는 제이지는 지도에서 도시 이름을 검색하면 해당 도시와 관련된 맛집 게시물들을 데이터베이스에서 읽어 보여주는 서비스를 개발하고 있다. 이 프로그램의 테스팅 업무를 담당하고 있는 어피치는 서비스를 오픈하기 전 각 로직에 대한 성능 측정을 수행하였는데, 제이지가 작...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Mar 15, 2020,  3:00 PM +0800"
  >

  
  

  
    Mar 15, 2020
  

  <i class="unloaded">2020-03-15T15:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="두번째 카카오 알고리즘을 풀어보겠습니다.

LRU알고리즘(난이도 하)

지도개발팀에서 근무하는 제이지는 지도에서 도시 이름을 검색하면 해당 도시와 관련된 맛집 게시물들을 데이터베이스에서 읽어 보여주는 서비스를 개발하고 있다. 이 프로그램의 테스팅 업무를 담당하고 있는 어피치는 서비스를 오픈하기 전 각 로직에 대한 성능 측정을 수행하였는데, 제이지가 작성한 부분 중 데이터베이스에서 게시물을 가져오는 부분의 실행시간이 너무 오래 걸린다는 것을 알게 되었다. 어피치는 제이지에게 해당 로직을 개선하라고 닦달하기 시작하였고, 제이지는 DB 캐시를 적용하여 성능 개선을 시도하고 있지만 캐시 크기를 얼마로 해야 효율적인지 몰라 난감한 상황이다.

어피치에게 시달리는 제이지를 도와, DB 캐시를 적용할 때 캐시 크기에 따른 실행시간 측정 프로그램을 작성하시오.


  
    입력 형식
캐시 크기(cacheSize)와 도시이름 배열(cities)을 입력받는다.
cacheSize는 정수이며, 범위는 0 ≦ cacheSize ≦ 30 이다.
cities는 도시 이름으로 이뤄진 문자열 배열로, 최대 도시 수는 100,000개이다.
각 도시 이름은 공백, 숫자, 특수문자 등이 없는 영문자로 구성되며, 대소문자 구분을 하지 않는다. 도시 이름은 최대 20자로 이루어져 있다.
  
  
    출력 형식
입력된 도시이름 배열을 순서대로 처리할 때, “총 실행시간”을 출력한다.
  
  
    조건
캐시 교체 알고리즘은 LRU(Least Recently Used)를 사용한다.
cache hit일 경우 실행시간은 1이다. 
cache miss일 경우 실행시간은 5이다.
  
  
    입출력 예제
  




1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
size=int(input('크기를 입력하세요 '))
put = [y.lower() for y in input('도시이름을 입력하세요 ').split(',')]

my_cache=[]
cnt=0

for i in put:
    if i in my_cache:
        my_cache.append(i)
        cnt+=1
    else:
        my_cache.append(i)
        cnt+=5

    if size &lt; len(my_cache):
        del my_cache[0]

    elif size == 0:
        my_cache.append(i)

 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/algorithm-kakao1/">2017 카카오 신입 공채 블라인드 시험문제(비밀지도)</a>
    </h1>

    <div class="post-content">
      <p>
        





        옛날 카카오 블라인드 알고리즘 문제를 한번 오랜만에 풀어볼까합니다.

비밀지도(난이도 하)

네오는 평소 프로도가 비상금을 숨겨놓는 장소를 알려줄 비밀지도를 손에 넣었다. 그런데 이 비밀지도는 숫자로 암호화되어 있어 위치를 확인하기 위해서는 암호를 해독해야 한다. 다행히 지도 암호를 해독할 방법을 적어놓은 메모도 함께 발견했다.

지도는 한 변의 길이...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Mar 15, 2020,  2:00 PM +0800"
  >

  
  

  
    Mar 15, 2020
  

  <i class="unloaded">2020-03-15T14:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="옛날 카카오 블라인드 알고리즘 문제를 한번 오랜만에 풀어볼까합니다.

비밀지도(난이도 하)

네오는 평소 프로도가 비상금을 숨겨놓는 장소를 알려줄 비밀지도를 손에 넣었다. 그런데 이 비밀지도는 숫자로 암호화되어 있어 위치를 확인하기 위해서는 암호를 해독해야 한다. 다행히 지도 암호를 해독할 방법을 적어놓은 메모도 함께 발견했다.

지도는 한 변의 길이가 n인 정사각형 배열 형태로, 각 칸은 “공백”(“ “) 또는 “벽”(“#”) 두 종류로 이루어져 있다.
전체 지도는 두 장의 지도를 겹쳐서 얻을 수 있다. 각각 “지도 1”과 “지도 2”라고 하자. 지도 1 또는 지도 2 중 어느 하나라도 벽인 부분은 전체 지도에서도 벽이다. 지도 1과 지도 2에서 모두 공백인 부분은 전체 지도에서도 공백이다.
“지도 1”과 “지도 2”는 각각 정수 배열로 암호화되어 있다.
암호화된 배열은 지도의 각 가로줄에서 벽 부분을 1, 공백 부분을 0으로 부호화했을 때 얻어지는 이진수에 해당하는 값의 배열이다.



네오가 프로도의 비상금을 손에 넣을 수 있도록, 비밀지도의 암호를 해독하는 작업을 도와줄 프로그램을 작성하라.


  입력 형식
입력으로 지도의 한 변 크기 n 과 2개의 정수 배열 arr1, arr2가 들어온다. 1 ≦ n ≦ 16


arr1, arr2는 길이 n인 정수 배열로 주어진다.정수 배열의 각 원소 x를 이진수로 변환했을 때의 길이는 n 이하이다. 즉, 0 ≦ x ≦ 2^n - 1을 만족한다.


  출력 형식


원래의 비밀지도를 해독하여 “#”, 공백으로 구성된 문자열 배열로 출력하라.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
num = int(input('n 값을 입력하세요 '))

arr1 = [int(x) for x in input('1번 지도를 입력하세요 ').split(',')]
arr2 = [int(y) for y in input('2번 지도를 입력하세요 ').split(',')]

total=[]

for i in arr1:    
    for j in arr2:

        a=bin(i | j)
        a = a.replace('0b','')

        if num &gt; len(a):
            a = (num-len(a))*str('0')+str(a)
            a = a.replace('1','#') 
            a = a.replace('0',' ') 
            total.append(a)

        else:
            a = a.replace('1','#') 
            a = a.replace('0',' ') 
            total.append(a)

if num != 0:       
    print(total[::num+1])     


조금 더 정성스럽게 풀엇다면 간단하고 이쁜 코드가 나왔겠지만….. 일단 대충합시다..
 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/ALGORITHM-coin-greedy/">파이썬으로 탐욕 알고리즘 구현하기(동전)</a>
    </h1>

    <div class="post-content">
      <p>
        





        탐욕 알고리즘은 당장 눈앞의 이익만 추구하는 것을 이야기 하며, 먼 미래를 내다 보지 않고 지금 당장의 최선이 무엇인가를 판단하는 알고리즘입니다.

그럼 탐욕 알고리즘을 이용하여 금액과 화폐가 주어졌을 때 가장 적은 화폐로 지불하는 것을 구현해도록 하겠습니다.

예를 들어 액수를 362 라고 입력하고 화폐단위를 1,50,100 이라고 입력했을 때 결과...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sat, Mar 14, 2020,  7:00 PM +0800"
  >

  
  

  
    Mar 14, 2020
  

  <i class="unloaded">2020-03-14T19:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="탐욕 알고리즘은 당장 눈앞의 이익만 추구하는 것을 이야기 하며, 먼 미래를 내다 보지 않고 지금 당장의 최선이 무엇인가를 판단하는 알고리즘입니다.

그럼 탐욕 알고리즘을 이용하여 금액과 화폐가 주어졌을 때 가장 적은 화폐로 지불하는 것을 구현해도록 하겠습니다.

예를 들어 액수를 362 라고 입력하고 화폐단위를 1,50,100 이라고 입력했을 때 결과 값이 100원 3개, 50원 1개, 1원 12개의 값이 나와야 합니다.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
def coinGreedy(): 
    money = int(input('액수입력 : '))
    cash_type = [int(x) for x in input('화폐단위를 입력하세요 : ').split(' ')]
    cash_type=sorted(cash_type, reverse=True)
    coin={}
    for i in cash_type:
        cnt=0
        while money &gt;= i:
            money=money-i
            cnt += 1
        coin[i]=cnt

    for key in coin:
        print('{0}원 : {1}개'.format(key,coin[key]))
    
coinGreedy()


구현하면 위와 같이 구현할 수 있고 탐욕스럽게 잘 나오고 있는 것을 알 수 있습니다.
 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/ALGORITHM-bubble_search/">파이썬으로 버블정렬 알고리즘 구현하기</a>
    </h1>

    <div class="post-content">
      <p>
        





        버블정렬은 두 인접한 원소를 검사하여 정렬하는 방법입니다.



위키백과 예제

파이썬으로 구현하자면 아래와 같이 구현할 수 있습니다.

def bubble_search(data):
    for i in range(len(data)):
        for j in range(len(data)-1):
            if data[j] &gt;...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sat, Mar 14, 2020,  6:00 PM +0800"
  >

  
  

  
    Mar 14, 2020
  

  <i class="unloaded">2020-03-14T18:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="버블정렬은 두 인접한 원소를 검사하여 정렬하는 방법입니다.



위키백과 예제

파이썬으로 구현하자면 아래와 같이 구현할 수 있습니다.

1
2
3
4
5
6
7
8
9
10
def bubble_search(data):
    for i in range(len(data)):
        for j in range(len(data)-1):
            if data[j] &gt; data[j+1]:
                (data[j],data[j+1]) = (data[j+1],data[j])
            print('i=',i,'j=',j,data) 
    print(data)

data = [5,4,3,2,1,8,7,10]
bubble_search(data)


여기서 버블 정렬을 재귀 함수로 구현한다면 아래와 같이 구현할 수 있습니다.

1
2
3
4
5
6
7
8
9
10
11
12
13
def bubble_search(data):
    for i in range(len(data)-1):
        if data[i] &gt; data[i+1]:
            a = data[i]
            data[i] = data[i+1]
            data[i+1] = a            
            bubble_search(data)
            
    return data

data = [5,4,3,2,1,8,7,10]
print(data)
print(bubble_search(data))

 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



  <div class="post-preview">
    <h1>
      <a href="/posts/python_val_pass/">파이썬에서 언더바(_)를 사용하는 경우</a>
    </h1>

    <div class="post-content">
      <p>
        





        파이썬 코드를 가끔 보다보면 _ 같은것을 볼 수 있습니다. 이 언더바(_)는 언제 사용하는 걸까요?

언더바(_)를 사용하는 경우

언더바를 사용하는 경우는 주로 아래의 4가지 경우입니다.


  인터프리터에서 마지막 값을 저장할 때
  값을 무시하고 싶을 때
  변수나 함수명에 특별한 의미를 부여하고 싶을 때
  숫자 또는 문자값의 자릿수 구분을 위...
      </p>
    </div>

    <div class="post-meta text-muted d-flex justify-content-between">

      <div>
        <!-- posted date -->
        <i class="far fa-calendar fa-fw"></i>
        <!--
  Date format snippet
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sat, Mar 14, 2020,  5:00 PM +0800"
  >

  
  

  
    Mar 14, 2020
  

  <i class="unloaded">2020-03-14T17:00:00+08:00</i>

</span>


        <!-- time to read -->
        <!-- <i class="far fa-clock fa-fw"></i> -->
        <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<!-- <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="파이썬 코드를 가끔 보다보면 _ 같은것을 볼 수 있습니다. 이 언더바(_)는 언제 사용하는 걸까요?

언더바(_)를 사용하는 경우

언더바를 사용하는 경우는 주로 아래의 4가지 경우입니다.


  인터프리터에서 마지막 값을 저장할 때
  값을 무시하고 싶을 때
  변수나 함수명에 특별한 의미를 부여하고 싶을 때
  숫자 또는 문자값의 자릿수 구분을 위한 구분자로써 사용할 때


우선 첫번째, 인터프리터에서 마지막 값을 저장 할때 사용하는데, 아래와 같이 사용할 수 있습니다.



그래서 이런 출력도 가능합니다.



두번째는 값을 무시하고 싶을 때 사용하는데, for 문을 사용할 때 가장 많이 사용됩니다.



2를 무시하고 값이 출력되는 것을 알 수 있으며, for 문을 사용하면 아래와 같이 사용할 수있습니다.



세번짼 변수나 함수명에 특별한 의미를 부여하고 싶을 때 사용합니다. 패키지를 만들기 위해 폴더 안에 init 파일 만들때 사용합니다. 또한 함수명에 언더바를 붙이는 것에 따라 두가지 의미를 가지 수 있습니다.

1
2
3
4
5
6
class Message:
    def __init__(self, msg):
        self.msg=msg
        
    def __repr__(self):
return "Message: %s" %self.msg



  ‘_‘이 앞에 붙으면 외부 사용자는 사용하지말라는 권유의 문법입니다.
  ‘__‘이 앞에 붙으면 private이 되어 외부에서 사용할 수 없고, 다른 클래스에서 사용하거나 override 할 수 없습니다.


__를 붙인 함수와 안붙인 함수는 두가지 차이점을 가지고 있습니다.


  이 클래스 파일을 다른 파일에 import 할 때, __가 붙은 함수는 import가 되지 않습니다.
  클래스를 인스턴스화 하고 함수를 호출할때, 객체명만 사용하고 함수이름을 쓰지 않아도 실행을 할 수 있습니다.


마지막으로 언더바는 숙자 또는 문자값의 자릿수 구분을 위한 구분자로써 사용할 수 있는데, 아래와 같이 사용할 수 있습니다.

1
2
dec_base= 1_000_000 
print(dec_base)



 words">1 min</span> -->


        <!-- page views -->
        
      </div>

      

    </div> <!-- .post-meta -->

  </div> <!-- .post-review -->



</div> <!-- #post-list -->


  <!--
  The paginator for post list on HomgPage.
-->

<ul class="pagination mt-4 mb-0 pl-lg-2">
  <!-- left arrow -->
  
  <li class="page-item ">
    <a class="page-link btn-box-shadow" href="/page6" aria-label="previous-page">
      <i class="fas fa-angle-left"></i>
    </a>
  </li>

  <!-- page numbers -->
  
  

  

    
    
    
    
    

    

    
      <!-- show number -->
      <li class="page-item ">
        <a class="page-link btn-box-shadow" href="/">1</a>
      </li>
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
        <li class="page-item disabled">
          <span class="page-link btn-box-shadow">...</span>
        </li>
        
      
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
    

  

    
    
    
    
    

    

    
      <!-- show number -->
      <li class="page-item ">
        <a class="page-link btn-box-shadow" href="/page6/">6</a>
      </li>
    

  

    
    
    
    
    

    

    
      <!-- show number -->
      <li class="page-item  active">
        <a class="page-link btn-box-shadow" href="/page7/">7</a>
      </li>
    

  

    
    
    
    
    

    

    
      <!-- show number -->
      <li class="page-item ">
        <a class="page-link btn-box-shadow" href="/page8/">8</a>
      </li>
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
        <li class="page-item disabled">
          <span class="page-link btn-box-shadow">...</span>
        </li>
        
      
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
    

  

    
    
    
    
    

    

    
      <!-- hide number -->
      
    

  

    
    
    
    
    

    

    
      <!-- show number -->
      <li class="page-item ">
        <a class="page-link btn-box-shadow" href="/page14/">14</a>
      </li>
    

  

  <!-- right arrow -->
  
  <li class="page-item ">
    <a class="page-link btn-box-shadow" href="/page8" aria-label="next-page">
      <i class="fas fa-angle-right"></i>
    </a>
  </li>

</ul> <!-- .pagination -->



    
    </div> <!-- #page -->
  </div><!-- .col-12 -->

  <!--
  The Pannel on right side (Desktop views)
-->


</div>
<!-- 
 -->





        <!--
  The Footer
-->

<footer class="d-flex w-100 justify-content-center">
  <div class="d-flex justify-content-between align-items-center">
    <div class="footer-left">
      <p class="mb-0">
        © 2022
        <a href="https://www.instagram.com/ao_ej125">kejdev</a>.
        
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
        
      </p>
    </div>

    <div class="footer-right">
      <p class="mb-0">
        Powered by
        <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
        with
        <a href="https://github.com/cotes2020/jekyll-theme-chirpy"
          target="_blank" rel="noopener">Chirpy</a>
        theme.
      </p>
    </div>

  </div> <!-- div.d-flex -->
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-xl-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



      
        
        <a class="post-tag" href="/tags/python/">Python</a>
      
        
        <a class="post-tag" href="/tags/%EB%AC%B8%EB%B2%95/">문법</a>
      
        
        <a class="post-tag" href="/tags/%ED%86%B5%EA%B3%84/">통계</a>
      
        
        <a class="post-tag" href="/tags/ml/">ML</a>
      
        
        <a class="post-tag" href="/tags/r/">R</a>
      
        
        <a class="post-tag" href="/tags/%EB%85%BC%EB%AC%B8/">논문</a>
      
        
        <a class="post-tag" href="/tags/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/">알고리즘</a>
      
        
        <a class="post-tag" href="/tags/db/">DB</a>
      
        
        <a class="post-tag" href="/tags/go/">GO</a>
      
        
        <a class="post-tag" href="/tags/oracle/">Oracle</a>
      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="http://localhost:4000{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    <div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div>    <div><i class="fa fa-tag fa-fw"></i>{tags}</div>  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>'
});
</script>


  </body>

</html>

